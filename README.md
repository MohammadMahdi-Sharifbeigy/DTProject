<div style="font-family: 'IRANSans', 'Tahoma', sans-serif;">

## مرور کلی

این مستندات پیاده‌سازی سفارشی طبقه‌بندی اشیاء نجومی با استفاده از درختان تصمیم را پوشش می‌دهد که به طور خاص برای مجموعه داده‌های بررسی دیجیتال آسمان اسلون (SDSS) طراحی شده است. این پیاده‌سازی بر موارد زیر تأکید دارد:

1. پیش‌پردازش داده‌های مختص نجوم
2. مهندسی ویژگی برای خصوصیات اخترفیزیکی
3. پیاده‌سازی سفارشی درخت تصمیم
4. تکنیک‌های ارزیابی مدل
5. تحلیل اهمیت ویژگی‌ها

## فهرست مطالب

1. [بارگذاری و بررسی داده‌ها](#بارگذاری-و-بررسی-داده‌ها)
2. [مهندسی ویژگی‌ها](#مهندسی-ویژگی‌ها)
3. [پیاده‌سازی‌های پیش‌پردازش سفارشی](#پیاده‌سازی‌های-پیش‌پردازش-سفارشی)
   - [CustomPCA](#custompca)
   - [RobustScaler](#robustscaler)
   - [SelectKBest](#selectkbest)
4. [توابع پیش‌پردازش داده‌ها](#توابع-پیش‌پردازش-داده‌ها)
5. [پیاده‌سازی درخت تصمیم](#پیاده‌سازی-درخت-تصمیم)
6. [ارزیابی مدل](#ارزیابی-مدل)
---

 نویسنده: [محمدمهدی شریف‌بیگی][(sharifbeigymohammad@gmail.com)](https://github.com/MohammadMahdi-Sharifbeigy)

 تاریخ: [25/3/2025]

 </div>

<div style="font-family: 'IRANSans', 'Tahoma', sans-serif;">

## بارگذاری و بررسی داده‌ها

### `load_and_explore_data(filepath, verbose=True)`

داده‌های نجومی را بارگذاری کرده و بررسی اولیه را انجام می‌دهد.

**پارامترها:**
- `filepath`: مسیر فایل CSV مجموعه داده SDSS
- `verbose`: آیا اطلاعات بررسی چاپ شود

**خروجی:**
- دیتافریم با داده‌های بارگذاری شده

**توضیحات:**
مجموعه داده SDSS معمولاً شامل اندازه‌گیری‌های نجومی زیر است:
- `objid`، `specobjid`: شناسه‌های اشیاء در کاتالوگ SDSS
- `ra`، `dec`: میل و بعد (مختصات آسمانی)
- `u`، `g`، `r`، `i`، `z`: قدرهای ظاهری در پنج باند فوتومتری
- `redshift`: انتقال به سرخ شیء، مرتبط با فاصله/سرعت دور شدن
- `class`: طبقه‌بندی شیء (کهکشان، کوازار/QSO، یا ستاره)

---
</div>

<div dir="rtl" style="font-family: 'IRANSans', 'Tahoma', sans-serif;">

## Feature Engineering

### `engineer_astronomical_features(df)`

ویژگی‌های مبتنی بر فیزیک با استفاده از دانش نجومی ایجاد می‌کند.

**پارامترها:**
- `df`: دیتافریم با اندازه‌گیری‌های اصلی نجومی

**خروجی:**
- دیتافریم بهبود یافته با ویژگی‌های مهندسی شده

**ویژگی‌های ایجاد شده:**
1. **شاخص‌های رنگی**: تفاوت بین قدرهای ظاهری در باندهای مجاور
   - `u_g = u - g` (فرابنفش منهای سبز)
   - `g_r = g - r` (سبز منهای قرمز)
   - `r_i = r - i` (قرمز منهای مادون قرمز نزدیک)
   - `i_z = i - z` (مادون قرمز نزدیک منهای مادون قرمز)

2. **ویژگی‌های مشتق شده از انتقال به سرخ**:
   - `log_redshift = log(1 + redshift)`
   - `distance_modulus = 5 * log10(redshift * c / H0) + 25`

---
</div>

<div style="font-family: 'IRANSans', 'Tahoma', sans-serif;">

## Custom Preprocessing Implementations

### CustomPCA

پیاده‌سازی سفارشی تحلیل مؤلفه‌های اصلی برای کاهش ابعاد.

#### `__init__(n_components=None)`
PCA را مقداردهی اولیه می‌کند.

**پارامترها:**
- `n_components`: تعداد مؤلفه‌هایی که باید حفظ شوند

#### `fit(X)`
مدل PCA را با X تطبیق می‌دهد.

**پارامترها:**
- `X`: داده‌های آموزشی

#### `transform(X)`
داده‌ها را روی مؤلفه‌های اصلی تصویر می‌کند.

**پارامترها:**
- `X`: داده‌هایی که باید تبدیل شوند

#### `inverse_transform(X)`
داده‌ها را به فضای اصلی برمی‌گرداند.

**پارامترها:**
- `X`: داده‌های با ابعاد کاهش یافته

#### `fit_transform(X)`
تطبیق و تبدیل را در یک مرحله ترکیب می‌کند.

#### فرمول‌بندی ریاضی پیشرفته

با فرض یک مجموعه داده نجومی با $n$ مشاهده و $d$ ویژگی (مانند قدرهای ظاهری، رنگ‌ها، شاخص‌های طیفی)، نمایش داده شده به صورت ماتریس $\mathbf{X} \in \mathbb{R}^{n \times d}$، PCA به دنبال یافتن نمایشی با ابعاد کمتر است که بیشترین واریانس را حفظ کند.

##### مرکززدایی ماتریس

ماتریس داده مرکززدایی شده $\mathbf{X}_c$ با کم کردن میانگین هر ویژگی محاسبه می‌شود:

$\mathbf{X}_c = \mathbf{X} - \mathbf{1}\boldsymbol{\mu}^T$

که در آن $\boldsymbol{\mu} \in \mathbb{R}^d$ بردار میانگین با عناصر $\mu_j = \frac{1}{n}\sum_{i=1}^{n} x_{ij}$ است و $\mathbf{1} \in \mathbb{R}^n$ یک بردار از یک‌ها است.

##### ماتریس کوواریانس

ماتریس کوواریانس نمونه $\mathbf{\Sigma}$ به صورت زیر تعریف می‌شود:

$\mathbf{\Sigma} = \frac{1}{n-1}\mathbf{X}_c^T\mathbf{X}_c \in \mathbb{R}^{d \times d}$

هر عنصر $\sigma_{jk}$ نشان‌دهنده کوواریانس بین ویژگی‌های $j$ و $k$ است:

$\sigma_{jk} = \frac{1}{n-1}\sum_{i=1}^{n}(x_{ij} - \mu_j)(x_{ik} - \mu_k)$

برای داده‌های نجومی، این روابط بین باندهای طول موج مختلف یا خصوصیات مشتق شده را نشان می‌دهد.

##### روش تجزیه ویژه مقدار

ماتریس کوواریانس به مقادیر ویژه و بردارهای ویژه خود تجزیه می‌شود:

$\mathbf{\Sigma} = \mathbf{V}\mathbf{\Lambda}\mathbf{V}^T$

که در آن:
- $\mathbf{V} \in \mathbb{R}^{d \times d}$ ماتریس متعامد بردارهای ویژه $\mathbf{v}_1, \mathbf{v}_2, ..., \mathbf{v}_d$ است
- $\mathbf{\Lambda} = \text{diag}(\lambda_1, \lambda_2, ..., \lambda_d)$ ماتریس قطری مقادیر ویژه است
- مقادیر ویژه به صورت نزولی مرتب شده‌اند: $\lambda_1 \geq \lambda_2 \geq ... \geq \lambda_d \geq 0$

هر بردار ویژه $\mathbf{v}_j$ و مقدار ویژه متناظر آن $\lambda_j$ در رابطه زیر صدق می‌کنند:

$\mathbf{\Sigma}\mathbf{v}_j = \lambda_j\mathbf{v}_j$

##### روش تجزیه مقدار تکین (SVD)

روش جایگزین و از نظر عددی پایدارتر، استفاده از SVD است که مستقیماً ماتریس داده مرکززدایی شده را تجزیه می‌کند:

$\mathbf{X}_c = \mathbf{U}\mathbf{S}\mathbf{V}^T$

که در آن:
- $\mathbf{U} \in \mathbb{R}^{n \times n}$ یک ماتریس متعامد حاوی بردارهای تکین چپ است
- $\mathbf{S} \in \mathbb{R}^{n \times d}$ یک ماتریس قطری مستطیلی با مقادیر تکین $s_1 \geq s_2 \geq ... \geq s_{\\min(n,d)} \geq 0$ است
- $\mathbf{V} \in \mathbb{R}^{d \times d}$ یک ماتریس متعامد حاوی بردارهای تکین راست است

رابطه بین SVD و تجزیه ویژه مقدار:
- بردارهای تکین راست در $\mathbf{V}$ با بردارهای ویژه $\mathbf{\Sigma}$ یکسان هستند
- مقادیر تکین و مقادیر ویژه با هم مرتبط هستند: $s_i^2 = (n-1) \lambda_i$

##### مؤلفه‌های اصلی و کاهش ابعاد

مؤلفه‌های اصلی با تصویر داده‌های مرکززدایی شده روی بردارهای ویژه به دست می‌آیند:

$\mathbf{T} = \mathbf{X}_c\mathbf{V} \in \mathbb{R}^{n \times d}$

برای کاهش ابعاد به $k < d$ مؤلفه، فقط از $k$ بردار ویژه اول استفاده می‌کنیم:

$\mathbf{T}_k = \mathbf{X}_c\mathbf{V}_k \in \mathbb{R}^{n \times k}$

که در آن $\mathbf{V}_k \in \mathbb{R}^{d \times k}$ شامل $k$ بردار ویژه اول است.

##### واریانس توضیح داده شده

نسبت واریانس توضیح داده شده توسط $j$-امین مؤلفه اصلی برابر است با:

$\text{نسبت واریانس توضیح داده شده}_j = \frac{\lambda_j}{\sum_{i=1}^{d}\lambda_i}$

واریانس تجمعی توضیح داده شده برای $k$ مؤلفه برابر است با:

$\text{واریانس تجمعی توضیح داده شده}_k = \frac{\sum_{j=1}^{k}\lambda_j}{\sum_{i=1}^{d}\lambda_i}$

برای داده‌های نجومی، این به تعیین تعداد مؤلفه‌های مورد نیاز برای حفظ اطلاعات اساسی کمک می‌کند.

##### بازسازی داده‌ها

داده‌های اصلی می‌توانند به طور تقریبی از نمایش کاهش یافته بازسازی شوند:

$\hat{\mathbf{X}} = \mathbf{T}_k\mathbf{V}_k^T + \mathbf{1}\boldsymbol{\mu}^T$

خطای بازسازی با میانگین مربعات خطا کمی می‌شود:

$\text{MSE} = \frac{1}{nd}\\|\mathbf{X} - \hat{\mathbf{X}}\\|_F^2 = \frac{1}{nd}\sum_{j=k+1}^{d}\lambda_j$

که در آن $\\|\cdot\\|_F$ نُرم فروبنیوس است.

##### مثال ماتریسی

یک مجموعه داده نجومی ساده با 4 شیء و 3 ویژگی (قدرهای ظاهری در باندهای مختلف) را در نظر بگیرید:

$\mathbf{X} = \begin{bmatrix} 
2.1 & 1.5 & 3.1 \\
3.5 & 2.7 & 4.2 \\
1.8 & 1.3 & 2.8 \\
2.6 & 2.0 & 3.5
\end{bmatrix}$

**گام 1**: محاسبه بردار میانگین

$\boldsymbol{\mu} = \begin{bmatrix} 2.5 & 1.875 & 3.4 \end{bmatrix}$

**گام 2**: مرکززدایی داده‌ها

$\mathbf{X}_c = \begin{bmatrix} 
-0.4 & -0.375 & -0.3 \\
1.0 & 0.825 & 0.8 \\
-0.7 & -0.575 & -0.6 \\
0.1 & 0.125 & 0.1
\end{bmatrix}$

**گام 3**: محاسبه ماتریس کوواریانس

$\mathbf{\Sigma} = \frac{1}{3}\mathbf{X}_c^T\mathbf{X}_c = \begin{bmatrix} 
0.517 & 0.433 & 0.467 \\
0.433 & 0.365 & 0.392 \\
0.467 & 0.392 & 0.423
\end{bmatrix}$

**گام 4**: یافتن مقادیر ویژه و بردارهای ویژه

$\lambda_1 = 1.295, \quad \mathbf{v}_1 = \begin{bmatrix} 0.58 \\ 0.49 \\ 0.53 \end{bmatrix}$

$\lambda_2 = 0.010, \quad \mathbf{v}_2 = \begin{bmatrix} -0.54 \\ 0.83 \\ -0.15 \end{bmatrix}$

$\lambda_3 = 0.001, \quad \mathbf{v}_3 = \begin{bmatrix} 0.61 \\ 0.28 \\ -0.74 \end{bmatrix}$

**گام 5**: تصویر داده‌ها روی مؤلفه‌های اصلی (2 مؤلفه اول)

$\mathbf{T}_2 = \mathbf{X}_c \begin{bmatrix} 
0.58 & -0.54 \\
0.49 & 0.83 \\
0.53 & -0.15
\end{bmatrix} = \begin{bmatrix} 
-0.62 & 0.035 \\
1.52 & 0.122 \\
-1.08 & -0.026 \\
0.19 & -0.131
\end{bmatrix}$

مؤلفه اصلی اول 99.1% از واریانس را توضیح می‌دهد، که نشان می‌دهد سه ویژگی اصلی به شدت با هم همبستگی دارند.
</div>

<div dir="rtl" style="font-family: 'IRANSans', 'Tahoma', sans-serif;">

# RobustScaler: Scaling Features with Resilience to Outliers

## مرور کلی

`RobustScaler` یک تکنیک مقیاس‌دهی ویژگی‌ها است که برای مدیریت مجموعه داده‌هایی با داده‌های پرت قابل توجه طراحی شده است، که آن را به‌ویژه برای تحلیل داده‌های نجومی و علمی ارزشمند می‌سازد.

## ویژگی‌های کلیدی

- **مقاوم در برابر داده‌های پرت**: از میانه و دامنه میان‌چارکی (IQR) به جای میانگین و انحراف معیار استفاده می‌کند
- **مقیاس‌دهی انعطاف‌پذیر**: می‌تواند داده‌ها را به طور مستقل مرکززدایی و مقیاس‌دهی کند
- **محدوده چندک قابل تنظیم**: اجازه مشخص کردن محدوده‌های چندک سفارشی را می‌دهد

## پارامترها

### `with_centering` : bool، پیش‌فرض=True
- اگر `True` باشد، داده‌ها را با کم کردن میانه مرکززدایی می‌کند
- تأثیر مقادیر افراطی بر مقیاس‌دهی را کاهش می‌دهد

### `with_scaling` : bool، پیش‌فرض=True
- اگر `True` باشد، داده‌ها را با استفاده از دامنه میان‌چارکی مقیاس‌دهی می‌کند
- معیار مقاوم‌تری از پراکندگی داده‌ها ارائه می‌دهد

### `quantile_range` : tuple، پیش‌فرض=(25.0, 75.0)
- محدوده‌ای را برای محاسبه مقیاس تعریف می‌کند
- پیش‌فرض از کل دامنه میان‌چارکی (IQR) استفاده می‌کند
- فرمت: (چندک_پایین، چندک_بالا)

## ویژگی‌ها

### `center_` : ndarray
- مقادیر میانه برای هر ویژگی در مجموعه آموزش
- شکل: (n_features,)

### `scale_` : ndarray
- دامنه میان‌چارکی برای هر ویژگی
- شکل: (n_features,)

## فرمول تبدیل
X_scaled = (X - median) / (q_max - q_min)
که در آن:
- `X` ماتریس ورودی ویژگی‌ها است
- `median` میانه ویژگی به ویژگی است
- `q_max`، `q_min` مقادیر چندک مشخص شده هستند

## مثال

</div>

```python
import numpy as np
from custom_astro_ml import RobustScaler

X = np.array([
    [1., -2., 2.],     
    [2., 0., 0.],     
    [0., 1., -1.],    
    [1., 1., 1.],  
    [6., 10., 10.]     
])

transformer = RobustScaler().fit(X)

X_scaled = transformer.transform(X)

```

<div dir="rtl" style="font-family: 'IRANSans', 'Tahoma', sans-serif;">

پیچیدگی محاسباتی

تطبیق: O(n * d)، که n تعداد نمونه‌ها و d تعداد ویژگی‌ها است
تبدیل: O(n * d)

محدودیت‌ها

ممکن است برای داده‌های با توزیع کاملاً نرمال ایده‌آل نباشد
عملکرد به ویژگی‌های خاص مجموعه داده شما بستگی دارد
</div>

<div dir="rtl" style="font-family: 'IRANSans', 'Tahoma', sans-serif;">

### SelectKBest

ویژگی‌ها را بر اساس k امتیاز بالاتر طبق تابع امتیازدهی انتخاب می‌کند.

#### `__init__(score_func=None, k=10)`
انتخابگر ویژگی را مقداردهی اولیه می‌کند.

**پارامترها:**
- `score_func`: تابعی برای امتیازدهی ویژگی‌ها
- `k`: تعداد ویژگی‌های برتر برای انتخاب

#### `fit(X, y)`
تابع امتیازدهی را اجرا کرده و ویژگی‌ها را انتخاب می‌کند.

**پارامترها:**
- `X`: ویژگی‌های آموزشی
- `y`: مقادیر هدف

#### `transform(X)`
X را به ویژگی‌های انتخاب شده کاهش می‌دهد.

**پارامترها:**
- `X`: داده‌های ورودی

#### توابع امتیازدهی
- `f_classif`: مقدار F آنالیز واریانس بین ویژگی و هدف

F = واریانس_بین_کلاس‌ها / واریانس_درون_کلاس‌ها

- `mutual_info_classif`: اطلاعات متقابل را اندازه‌گیری می‌کند

MI(X,Y) = Σ p(x,y) log(p(x,y) / (p(x)p(y)))

</div>

<div dir="rtl" style="font-family: 'IRANSans', 'Tahoma', sans-serif;">

## Data Preprocessing Functions

### `detect_and_handle_outliers(df, method='robust', columns=None, threshold=3.0)`

داده‌های پرت را در داده‌های نجومی تشخیص داده و به طور مناسب مدیریت می‌کند.

**پارامترها:**
- `df`: مجموعه داده حاوی اندازه‌گیری‌های نجومی
- `method`: روش تشخیص ('robust' یا 'zscore')
- `columns`: ستون‌های خاص برای بررسی
- `threshold`: آستانه برای تشخیص داده‌های پرت

**خروجی:**
- دیتافریم پردازش شده و آمارهای داده‌های پرت

**تکنیک‌ها:**
- **روش IQR**: `lower_bound = Q1 - threshold * IQR`، `upper_bound = Q3 + threshold * IQR`
- **روش نمره Z**: داده‌های پرت جایی که `|z| > threshold`

**مدیریت داده‌های پرت:** وینسوریزاسیون (جایگزینی داده‌های پرت با مقادیر مرزی)

### `select_important_features(X, y, method='mutual_info', k=15)`

مهم‌ترین ویژگی‌ها را برای طبقه‌بندی انتخاب می‌کند.

**پارامترها:**
- `X`: ماتریس ویژگی‌ها
- `y`: متغیر هدف
- `method`: روش انتخاب ویژگی
- `k`: تعداد ویژگی‌های برتر برای انتخاب

**خروجی:**
- ماتریس ویژگی‌های انتخاب شده، شاخص‌ها و شیء انتخابگر

### `apply_robust_scaling(X_train, X_test, method='robust')`

مقیاس‌دهی مناسب را به ویژگی‌های نجومی اعمال می‌کند.

**پارامترها:**
- `X_train`، `X_test`: داده‌های آموزش و آزمون
- `method`: روش مقیاس‌دهی ('robust'، 'standard' یا 'minmax')

**خروجی:**
- مجموعه داده‌های مقیاس‌دهی شده و شیء مقیاس‌دهنده

### `preprocess_astronomical_data(X_train, X_test, y_train, y_test, class_names=None)`

خط لوله جامع پیش‌پردازش برای داده‌های نجومی.

**پارامترها:**
- `X_train`، `X_test`: ماتریس‌های ویژگی
- `y_train`، `y_test`: متغیرهای هدف
- `class_names`: نام‌های کلاس‌ها

**خروجی:**
- مجموعه داده‌های پردازش شده و اطلاعات پیش‌پردازش

**مراحل :**
1. تشخیص و مدیریت داده‌های پرت
2. مقیاس‌دهی مقاوم
3. انتخاب ویژگی با استفاده از اطلاعات متقابل
4. کاهش ابعاد اختیاری

---
</div>

<div dir="rtl" style="font-family: 'IRANSans', 'Tahoma', sans-serif;">

## پیاده‌سازی درخت تصمیم

### کلاس: Node

یک گره در درخت تصمیم را نمایش می‌دهد.

**ویژگی‌ها:**
- `feature_index`: شاخص ویژگی برای انشعاب
- `thresholds`: مقادیر آستانه برای انشعاب
- `children`: گره‌های فرزند
- `info_gain`: بهره اطلاعاتی به دست آمده از انشعاب
- `value`: پیش‌بینی کلاس (برای گره‌های برگ)

### کلاس: AstronomicalDecisionTree

طبقه‌بندی کننده درخت تصمیم سفارشی بهینه‌سازی شده برای داده‌های نجومی.

#### `__init__(min_sample_split=2, max_depth=2, num_splits=2, criterion='entropy')`
درخت را مقداردهی اولیه می‌کند.

**پارامترها:**
- `min_sample_split`: حداقل نمونه‌ها برای انشعاب یک گره
- `max_depth`: حداکثر عمق درخت
- `num_splits`: تعداد انشعاب‌ها برای هر ویژگی
- `criterion`: معیار سنجش کیفیت انشعاب

#### `fit(X, y)`
درخت تصمیم را می‌سازد.

**پارامترها:**
- `X`: داده‌های آموزشی
- `y`: مقادیر هدف

#### `_build_tree(dataset, curr_depth=0)`
به صورت بازگشتی درخت را با انتخاب بهترین انشعاب‌ها می‌سازد.

#### `_get_best_split(dataset, num_samples, num_features)`
بهترین ویژگی و آستانه را برای انشعاب پیدا می‌کند.

#### `_information_gain(parent, children)`
بهره اطلاعاتی از یک انشعاب را محاسبه می‌کند.

**فرمول (آنتروپی):**
IG = H(parent) - Σ (|child_i|/|parent|) * H(child_i)
**فرمول (جینی):**
IG = G(parent) - Σ (|child_i|/|parent|) * G(child_i)

#### `_entropy(y)`
آنتروپی مقادیر هدف را محاسبه می‌کند.

**فرمول:**
H(y) = -Σ p_i * log2(p_i)

#### `_gini_index(y)`
شاخص جینی مقادیر هدف را محاسبه می‌کند.

**فرمول:**
G(y) = 1 - Σ p_i²

#### `predict(X)`
کلاس‌ها را برای نمونه‌های آزمون پیش‌بینی می‌کند.

#### `print_tree(tree=None, indent="", feature_names=None, class_names=None)`
نمایش قابل خواندن انسان از درخت را چاپ می‌کند.

#### `get_feature_importance(feature_names=None)`
اهمیت ویژگی‌ها را بر اساس بهره اطلاعاتی محاسبه می‌کند.

---
</div>

<div dir="rtl" style="font-family: 'IRANSans', 'Tahoma', sans-serif;">

## ارزیابی مدل

### `evaluate_tree_classifier(tree, X_train, y_train, X_test, y_test, class_names=None)`

یک طبقه‌بندی‌کننده درخت تصمیم را به طور جامع ارزیابی می‌کند.

**پارامترها:**
- `tree`: طبقه‌بندی‌کننده آموزش دیده
- `X_train`، `X_test`: ماتریس‌های ویژگی
- `y_train`، `y_test`: متغیرهای هدف
- `class_names`: نام‌های کلاس‌ها

**خروجی:**
- دیکشنری با معیارهای ارزیابی

**معیارهای ارزیابی:**
- دقت آموزش و آزمون
- ماتریس اغتشاش
- گزارش طبقه‌بندی (دقت، بازخوانی، امتیاز F1)
- تحلیل اهمیت ویژگی‌ها

### `visualize_decision_boundaries(tree, X, y, feature_idx1, feature_idx2, feature_names=None, class_names=None)`

مرزهای تصمیم‌گیری را در فضای ویژگی دوبعدی مصور می‌کند.

**پارامترها:**
- `tree`: طبقه‌بندی‌کننده آموزش دیده
- `X`، `y`: داده‌ها و مقادیر هدف
- `feature_idx1`، `feature_idx2`: شاخص‌های ویژگی‌ها برای نمایش
- `feature_names`، `class_names`: نام‌ها برای برچسب‌گذاری

**تکنیک مصورسازی:**
1. یک شبکه مش در فضای دو بعدی ایجاد می‌کند
2. برای هر نقطه، از مقادیر میانه برای سایر ویژگی‌ها استفاده می‌کند
3. کلاس را برای هر نقطه مش پیش‌بینی می‌کند
4. مرزهای تصمیم را به صورت نواحی رنگی نمایش می‌دهد
5. نقاط آموزش را روی آن قرار می‌دهد

---
</div>